{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2e732e2d-539d-49ee-8e4d-731177132c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime\n",
    "import time\n",
    "from urllib.parse import urlparse\n",
    "from utils import update_bq_table, generate_filename, upload_blob_from_string\n",
    "bucket_name = 'nhanes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "d7cf6606-e287-4833-b24f-f72c782a3fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_nhanes_table(soup, component):\n",
    "    table = soup.find('table',id='GridView1')\n",
    "\n",
    "    headers = []\n",
    "    for i in table.find_all('th'):\n",
    "        title = i.text\n",
    "        headers.append(title)\n",
    "\n",
    "    base_url = \"https://wwwn.cdc.gov\"\n",
    "\n",
    "    data = []\n",
    "\n",
    "    for j in table.find_all('tr')[1:]:\n",
    "        row_data = j.find_all('td')\n",
    "        row = [i.text for i in row_data] + [base_url + i.a['href'] for i in row_data if i.find('a')]\n",
    "        data.append(row)\n",
    "    \n",
    "    if len(data[0]) == len(headers) + 2:\n",
    "        headers = ['_'.join(h.lower().split()) for h in headers] + ['doc_file_url','data_file_url']\n",
    "    elif len(data[0]) == len(headers) + 1:\n",
    "        headers = ['_'.join(h.lower().split()) for h in headers] + ['doc_file_url']\n",
    "    else:\n",
    "        headers = ['_'.join(h.lower().split()) for h in headers]\n",
    "\n",
    "    df = pd.DataFrame(columns = headers,data=data)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4052d0-c7ff-47c7-bec9-2d5860beeff2",
   "metadata": {},
   "source": [
    "### Scrape Continuous NHANES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "6e0c7e34-79ed-4b5c-9fa9-3ea3239e2d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nhanes_continuous_file_metadata_20231107_203909.csv uploaded to nhanes / nhanes_continuous_file_metadata/\n",
      "Starting job b013b250-2144-4057-b9ed-cce75b93fdc2\n",
      "Job finished.\n",
      "Table Row Count 1737 rows.\n"
     ]
    }
   ],
   "source": [
    "dfs = []\n",
    "\n",
    "components = [\"Demographics\",\"Dietary\",\"Examination\",\"Laboratory\",\"Questionnaire\",\"LimitedAccess\"]\n",
    "\n",
    "for component in components: \n",
    "    r = requests.get(f\"https://wwwn.cdc.gov/nchs/nhanes/search/datapage.aspx?Component={component}\")\n",
    "\n",
    "    soup = BeautifulSoup(r.text, 'lxml')\n",
    "\n",
    "    table = soup.find('table',id='GridView1')\n",
    "\n",
    "    headers = []\n",
    "    for i in table.find_all('th'):\n",
    "        title = i.text\n",
    "        headers.append(title)\n",
    "    \n",
    "    if component != 'LimitedAccess':\n",
    "        headers = ['_'.join(h.lower().split()) for h in headers] + ['doc_file_url','data_file_url']\n",
    "    else:\n",
    "        headers = ['_'.join(h.lower().split()) for h in headers] + ['doc_file_url']\n",
    "\n",
    "    base_url = \"https://wwwn.cdc.gov\"\n",
    "\n",
    "    data = []\n",
    "\n",
    "    for j in table.find_all('tr')[1:]:\n",
    "        row_data = j.find_all('td')\n",
    "        row = [i.text for i in row_data] + [base_url + i.a['href'] for i in row_data if i.find('a')]\n",
    "        data.append(row)\n",
    "\n",
    "    df = pd.DataFrame(columns = headers,data=data)\n",
    "\n",
    "    df['years'] = df['years'].str.strip()\n",
    "    df['doc_file'] = df['doc_file'].str.strip()\n",
    "    df['data_file'] = df['data_file'].str.strip()\n",
    "    df['date_published'] = df['date_published'].str.strip().str.replace('Updated ','')\n",
    "    df['start_year'] = df['years'].apply(lambda x: x.split('-')[0]).astype(int)\n",
    "    df['end_year'] = df['years'].apply(lambda x: x.split('-')[1]).astype(int)\n",
    "    df['published_date'] = pd.to_datetime(df['date_published'],format='%B %Y',errors='ignore')\n",
    "    df['page_component'] = component\n",
    "    df['last_updated'] = datetime.datetime.utcnow()\n",
    "    df['gcs_doc_filename'] = (f'gs://{bucket_name}/docs/' + df['data_file_name'] + ' ' + df['start_year'].astype(str) + ' ' + df['end_year'].astype(str) + ' Documentation').apply(lambda x: generate_filename(x,extension='.html'))\n",
    "    df['gcs_data_filename'] = (f'gs://{bucket_name}/data/' +df['data_file_name'] + ' ' + df['start_year'].astype(str) + ' ' + df['end_year'].astype(str) + ' Data').apply(lambda x: generate_filename(x,extension='.XPT'))\n",
    "    \n",
    "    dfs.append(df)\n",
    "\n",
    "final_df = pd.concat(dfs,ignore_index=True)\n",
    "\n",
    "alias = 'nhanes_continuous_file_metadata'\n",
    "update_bq_table(final_df, alias, dataset='nhanes',bucket='nhanes', truncate = True, max_error=0, schema=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def21fd1-ae85-4033-b0a7-00cb7fd2b318",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d5195352-5bb1-4dfc-83b3-874356e60021",
   "metadata": {},
   "source": [
    "### Download Data and Docs to GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "35f0a64a-ad6c-4618-a5d6-99e1f6df7b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "for index, row in final_df.iterrows():\n",
    "    \n",
    "    if not row['doc_file_url'].endswith('.aspx'):\n",
    "        try:\n",
    "            r = requests.get(row['doc_file_url'],stream=True,timeout=20)\n",
    "            html_content = r.content\n",
    "            file_name = row['gcs_doc_filename']\n",
    "            upload_blob_from_string(bucket_name = bucket_name,\n",
    "                                    bucket_folder = 'docs/',\n",
    "                                    file_name = file_name,\n",
    "                                    blob_string = html_content,\n",
    "                                    encoding='text/html')\n",
    "        except Exception as ex:\n",
    "            print(ex)\n",
    "            print(f\"Unable to upload {row['gcs_doc_filename']}\")\n",
    "    else:\n",
    "        print(f\"Skipping {row['doc_file_url']}\")\n",
    "    \n",
    "\n",
    "    if not pd.isnull(row['data_file_url']) and not 'RDC' in row['data_file_url']:\n",
    "        try:\n",
    "            data_file_df = pd.read_sas(row['data_file_url'])\n",
    "            data_file_df.to_parquet(f\"gs://{bucket_name}/data/{row['gcs_data_filename'].replace('.XPT','.parquet')}\")\n",
    "            print(f\"{row['data_file_url']} uploaded to gs://{bucket_name}/data/\")\n",
    "        except Exception as ex:\n",
    "            print(ex)\n",
    "            print(f\"Unable to upload {row['gcs_data_filename']}\")\n",
    "            \n",
    "    else:\n",
    "        print(f\"Skipping {row['gcs_data_filename']}\")\n",
    "    \n",
    "print(f\"Entire process took {time.time() - start_time} seconds\")\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472f7d93-5d78-451d-ac65-75d84051b576",
   "metadata": {},
   "source": [
    "### Scrape Yearly Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "f885ce1e-59d3-4b8f-bbfd-ebe5777436c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nhanes_file_metadata_20231107_212632.csv uploaded to nhanes / nhanes_file_metadata/\n",
      "Starting job 07240954-8fc0-4ad3-a8e2-3d2d0f9b9c8c\n",
      "Job finished.\n",
      "Table Row Count 3578 rows.\n",
      "Entire process took 58.127763509750366 seconds\n"
     ]
    }
   ],
   "source": [
    "response = requests.get(f\"https://wwwn.cdc.gov/nchs/nhanes/default.aspx\")\n",
    "\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "ids = [tag['id'] for tag in soup.select('div[id]')]\n",
    "\n",
    "lis = soup.find('ul',id='nav-primary').find_all('li',{\"class\": \"list-group-item nav-lvl2\"})\n",
    "ids = [el.ul['id'] for el in lis if el.ul]\n",
    "ids = [i for i in ids if 'nhanes' in i]\n",
    "\n",
    "# ['nav-group-all-continuous-nhanes',\n",
    "#  'nav-group-nhanes-2021-2023',\n",
    "#  'nav-group-nhanes-2017-2020',\n",
    "#  'nav-group-nhanes-2019-2020']\n",
    "\n",
    "start_time = time.time()\n",
    "new_time = time.time()\n",
    "dfs = []\n",
    "\n",
    "base_url = \"https://wwwn.cdc.gov\"\n",
    "\n",
    "for nhanes_id in ids:\n",
    "    lis_to_scrape = soup.find('ul',id=nhanes_id).find_all('li')\n",
    "    for li in lis_to_scrape:\n",
    "        if 'Data' in li.text.strip() and 'Overview' not in li.text.strip() and 'Errata' not in li.text.strip() and 'Issues' not in li.text.strip() and 'Files' not in li.text.strip():\n",
    "            component_text = li.text.strip().replace('All ','').replace(' Data','')\n",
    "            url = base_url + li.a['href']\n",
    "            dataset = nhanes_id.replace('nav-group-','')\n",
    "            r = requests.get(url, timeout=20)\n",
    "            soup = BeautifulSoup(r.text)\n",
    "            df = scrape_nhanes_table(soup, component_text)\n",
    "            \n",
    "            columns = df.columns.tolist()\n",
    "            df['page_component'] = component_text\n",
    "            df['dataset'] = dataset\n",
    "            df['last_updated'] = datetime.datetime.utcnow()\n",
    "            \n",
    "            if 'years' in columns:\n",
    "                df['years'] = df['years'].str.strip()\n",
    "                df['start_year'] = df['years'].apply(lambda x: x.split('-')[0]).astype(int)\n",
    "                df['end_year'] = df['years'].apply(lambda x: x.split('-')[1]).astype(int)\n",
    "            if 'doc_file' in columns:\n",
    "                df['doc_file'] = df['doc_file'].str.strip()\n",
    "            if 'data_file' in columns:\n",
    "                df['data_file'] = df['data_file'].str.strip()\n",
    "            if 'date_published' in columns:\n",
    "                df['date_published'] = df['date_published'].str.strip().str.replace('Updated ','')\n",
    "                df['published_date'] = pd.to_datetime(df['date_published'],format='%B %Y',errors='ignore')\n",
    "            if 'data_file_name' in columns:\n",
    "                if 'continuous' in dataset:\n",
    "                    df['gcs_doc_filename'] = (df['data_file_name'] + ' ' + df['dataset'].astype(str) + ' ' + df['start_year'].astype(str) + ' ' + df['end_year'].astype(str) + ' Documentation').apply(lambda x: generate_filename(x,extension='.html'))\n",
    "                    df['gcs_data_filename'] = (df['data_file_name'] + ' ' + df['dataset'].astype(str) + ' ' + df['start_year'].astype(str) + ' ' + df['end_year'].astype(str) + ' Data').apply(lambda x: generate_filename(x,extension='.XPT'))\n",
    "                else:\n",
    "                    df['gcs_doc_filename'] = (df['data_file_name'] + ' ' + df['dataset'].astype(str) + ' Documentation').apply(lambda x: generate_filename(x,extension='.html'))\n",
    "                    df['gcs_data_filename'] = (df['data_file_name'] + ' ' + df['dataset'].astype(str) + ' Data').apply(lambda x: generate_filename(x,extension='.XPT'))\n",
    "                    \n",
    "                \n",
    "            dfs.append(df)\n",
    "        \n",
    "yearly_df = pd.concat(dfs,ignore_index=True)\n",
    "\n",
    "alias = 'nhanes_file_metadata'\n",
    "update_bq_table(yearly_df, alias, dataset='nhanes',bucket='nhanes', truncate = True, max_error=0, schema=None)\n",
    "\n",
    "print(f\"Entire process took {time.time() - start_time} seconds\")\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "1638f293-47e1-4aad-9ee0-0962913ef326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# yearly_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c947ffa-3c95-48c1-ab19-20fc5c115a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "demographic_variables_sample_weights_all_continuous_nhanes_2005_2006_documentation.html uploaded to nhanes / all-continuous-nhanes/docs/\n",
      "demographic_variables_sample_weights_all_continuous_nhanes_2005_2006_data.XPT uploaded to gs://nhanes/data/\n",
      "demographic_variables_sample_weights_all_continuous_nhanes_2007_2008_documentation.html uploaded to nhanes / all-continuous-nhanes/docs/\n",
      "demographic_variables_sample_weights_all_continuous_nhanes_2007_2008_data.XPT uploaded to gs://nhanes/data/\n",
      "demographic_variables_sample_weights_all_continuous_nhanes_2003_2004_documentation.html uploaded to nhanes / all-continuous-nhanes/docs/\n",
      "demographic_variables_sample_weights_all_continuous_nhanes_2003_2004_data.XPT uploaded to gs://nhanes/data/\n",
      "demographic_variables_sample_weights_all_continuous_nhanes_2001_2002_documentation.html uploaded to nhanes / all-continuous-nhanes/docs/\n",
      "demographic_variables_sample_weights_all_continuous_nhanes_2001_2002_data.XPT uploaded to gs://nhanes/data/\n",
      "demographic_variables_sample_weights_all_continuous_nhanes_1999_2000_documentation.html uploaded to nhanes / all-continuous-nhanes/docs/\n",
      "demographic_variables_sample_weights_all_continuous_nhanes_1999_2000_data.XPT uploaded to gs://nhanes/data/\n",
      "demographic_variables_sample_weights_all_continuous_nhanes_2009_2010_documentation.html uploaded to nhanes / all-continuous-nhanes/docs/\n",
      "demographic_variables_sample_weights_all_continuous_nhanes_2009_2010_data.XPT uploaded to gs://nhanes/data/\n",
      "demographic_variables_sample_weights_all_continuous_nhanes_2011_2012_documentation.html uploaded to nhanes / all-continuous-nhanes/docs/\n",
      "demographic_variables_sample_weights_all_continuous_nhanes_2011_2012_data.XPT uploaded to gs://nhanes/data/\n",
      "demographic_variables_and_sample_weights_all_continuous_nhanes_2013_2014_documentation.html uploaded to nhanes / all-continuous-nhanes/docs/\n",
      "demographic_variables_and_sample_weights_all_continuous_nhanes_2013_2014_data.XPT uploaded to gs://nhanes/data/\n",
      "demographic_variables_and_sample_weights_all_continuous_nhanes_2015_2016_documentation.html uploaded to nhanes / all-continuous-nhanes/docs/\n",
      "demographic_variables_and_sample_weights_all_continuous_nhanes_2015_2016_data.XPT uploaded to gs://nhanes/data/\n",
      "demographic_variables_and_sample_weights_all_continuous_nhanes_2017_2018_documentation.html uploaded to nhanes / all-continuous-nhanes/docs/\n",
      "demographic_variables_and_sample_weights_all_continuous_nhanes_2017_2018_data.XPT uploaded to gs://nhanes/data/\n",
      "demographic_variables_and_sample_weights_all_continuous_nhanes_2017_2020_documentation.html uploaded to nhanes / all-continuous-nhanes/docs/\n",
      "demographic_variables_and_sample_weights_all_continuous_nhanes_2017_2020_data.XPT uploaded to gs://nhanes/data/\n",
      "dietary_interview_individual_foods_all_continuous_nhanes_1999_2000_documentation.html uploaded to nhanes / all-continuous-nhanes/docs/\n",
      "dietary_interview_individual_foods_all_continuous_nhanes_1999_2000_data.XPT uploaded to gs://nhanes/data/\n",
      "dietary_interview_individual_foods_all_continuous_nhanes_2001_2002_documentation.html uploaded to nhanes / all-continuous-nhanes/docs/\n",
      "dietary_interview_individual_foods_all_continuous_nhanes_2001_2002_data.XPT uploaded to gs://nhanes/data/\n",
      "dietary_interview_individual_foods_first_day_all_continuous_nhanes_2003_2004_documentation.html uploaded to nhanes / all-continuous-nhanes/docs/\n",
      "dietary_interview_individual_foods_first_day_all_continuous_nhanes_2003_2004_data.XPT uploaded to gs://nhanes/data/\n",
      "dietary_interview_individual_foods_first_day_all_continuous_nhanes_2009_2010_documentation.html uploaded to nhanes / all-continuous-nhanes/docs/\n",
      "dietary_interview_individual_foods_first_day_all_continuous_nhanes_2009_2010_data.XPT uploaded to gs://nhanes/data/\n",
      "dietary_interview_individual_foods_first_day_all_continuous_nhanes_2007_2008_documentation.html uploaded to nhanes / all-continuous-nhanes/docs/\n",
      "dietary_interview_individual_foods_first_day_all_continuous_nhanes_2007_2008_data.XPT uploaded to gs://nhanes/data/\n",
      "dietary_interview_individual_foods_first_day_all_continuous_nhanes_2011_2012_documentation.html uploaded to nhanes / all-continuous-nhanes/docs/\n",
      "dietary_interview_individual_foods_first_day_all_continuous_nhanes_2011_2012_data.XPT uploaded to gs://nhanes/data/\n",
      "dietary_interview_individual_foods_first_day_all_continuous_nhanes_2005_2006_documentation.html uploaded to nhanes / all-continuous-nhanes/docs/\n",
      "dietary_interview_individual_foods_first_day_all_continuous_nhanes_2005_2006_data.XPT uploaded to gs://nhanes/data/\n",
      "dietary_interview_individual_foods_first_day_all_continuous_nhanes_2013_2014_documentation.html uploaded to nhanes / all-continuous-nhanes/docs/\n",
      "dietary_interview_individual_foods_first_day_all_continuous_nhanes_2013_2014_data.XPT uploaded to gs://nhanes/data/\n",
      "dietary_interview_individual_foods_first_day_all_continuous_nhanes_2015_2016_documentation.html uploaded to nhanes / all-continuous-nhanes/docs/\n",
      "dietary_interview_individual_foods_first_day_all_continuous_nhanes_2015_2016_data.XPT uploaded to gs://nhanes/data/\n",
      "dietary_interview_individual_foods_first_day_all_continuous_nhanes_2017_2018_documentation.html uploaded to nhanes / all-continuous-nhanes/docs/\n",
      "dietary_interview_individual_foods_first_day_all_continuous_nhanes_2017_2018_data.XPT uploaded to gs://nhanes/data/\n",
      "dietary_interview_individual_foods_first_day_all_continuous_nhanes_2017_2020_documentation.html uploaded to nhanes / all-continuous-nhanes/docs/\n",
      "dietary_interview_individual_foods_first_day_all_continuous_nhanes_2017_2020_data.XPT uploaded to gs://nhanes/data/\n",
      "dietary_interview_individual_foods_second_day_all_continuous_nhanes_2003_2004_documentation.html uploaded to nhanes / all-continuous-nhanes/docs/\n",
      "dietary_interview_individual_foods_second_day_all_continuous_nhanes_2003_2004_data.XPT uploaded to gs://nhanes/data/\n",
      "dietary_interview_individual_foods_second_day_all_continuous_nhanes_2009_2010_documentation.html uploaded to nhanes / all-continuous-nhanes/docs/\n",
      "dietary_interview_individual_foods_second_day_all_continuous_nhanes_2009_2010_data.XPT uploaded to gs://nhanes/data/\n",
      "dietary_interview_individual_foods_second_day_all_continuous_nhanes_2007_2008_documentation.html uploaded to nhanes / all-continuous-nhanes/docs/\n",
      "dietary_interview_individual_foods_second_day_all_continuous_nhanes_2007_2008_data.XPT uploaded to gs://nhanes/data/\n",
      "dietary_interview_individual_foods_second_day_all_continuous_nhanes_2011_2012_documentation.html uploaded to nhanes / all-continuous-nhanes/docs/\n",
      "dietary_interview_individual_foods_second_day_all_continuous_nhanes_2011_2012_data.XPT uploaded to gs://nhanes/data/\n",
      "dietary_interview_individual_foods_second_day_all_continuous_nhanes_2005_2006_documentation.html uploaded to nhanes / all-continuous-nhanes/docs/\n",
      "dietary_interview_individual_foods_second_day_all_continuous_nhanes_2005_2006_data.XPT uploaded to gs://nhanes/data/\n",
      "dietary_interview_individual_foods_second_day_all_continuous_nhanes_2013_2014_documentation.html uploaded to nhanes / all-continuous-nhanes/docs/\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "new_time = time.time()\n",
    "for index, row in yearly_df.iterrows():\n",
    "    \n",
    "    if not row['doc_file_url'].endswith('.aspx'):\n",
    "        try:\n",
    "            r = requests.get(row['doc_file_url'],stream=True,timeout=20)\n",
    "            html_content = r.content\n",
    "            file_name = row['gcs_doc_filename']\n",
    "            upload_blob_from_string(bucket_name = bucket_name,\n",
    "                                    bucket_folder = row['dataset']+'/docs/',\n",
    "                                    file_name = file_name,\n",
    "                                    blob_string = html_content,\n",
    "                                    encoding='text/html')\n",
    "        except Exception as ex:\n",
    "            print(ex)\n",
    "            print(f\"Unable to upload {row['gcs_doc_filename']}\")\n",
    "    else:\n",
    "        print(f\"Skipping {row['doc_file_url']}\")\n",
    "    \n",
    "\n",
    "    if not pd.isnull(row['data_file_url']) and not 'RDC' in row['data_file_url'] and row['gcs_data_filename'].lower().endswith('.xpt'):\n",
    "        try:\n",
    "            data_file_df = pd.read_sas(row['data_file_url'])\n",
    "            data_file_df.to_parquet(f\"gs://{bucket_name}/{row['dataset']}/data/{row['gcs_data_filename'].replace('.XPT','.parquet')}\")\n",
    "            print(f\"{row['gcs_data_filename']} uploaded to gs://{bucket_name}/{row['dataset']}/data/\")\n",
    "        except Exception as ex:\n",
    "            print(ex)\n",
    "            print(f\"Unable to upload {row['gcs_data_filename']}\")\n",
    "            \n",
    "    else:\n",
    "        print(f\"Skipping {row['gcs_data_filename']}\")\n",
    "        \n",
    "    if index % 100 == 0 and index > 0:\n",
    "        print(f\"Last 100 datasets completed in {time.time() - new_time()}\")\n",
    "        new_time = time.time()\n",
    "    \n",
    "print(f\"Entire process took {time.time() - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "ada1704f-7afd-465e-bb24-5e9bbea1223f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# table = soup.find('table',id='GridView1')\n",
    "\n",
    "# headers = []\n",
    "# for i in table.find_all('th'):\n",
    "#     title = i.text\n",
    "#     headers.append(title)\n",
    "\n",
    "# base_url = \"https://wwwn.cdc.gov\"\n",
    "\n",
    "# data = []\n",
    "\n",
    "# for j in table.find_all('tr')[1:]:\n",
    "#     row_data = j.find_all('td')\n",
    "#     row = [i.text for i in row_data] + [base_url + i.a['href'] for i in row_data if i.find('a')]\n",
    "#     data.append(row)\n",
    "    \n",
    "# if len(data[0]) == len(headers) + 2:\n",
    "#     headers = ['_'.join(h.lower().split()) for h in headers] + ['doc_file_url','data_file_url']\n",
    "# elif len(data[0]) == len(headers) + 1:\n",
    "#     headers = ['_'.join(h.lower().split()) for h in headers] + ['doc_file_url']\n",
    "# else:\n",
    "#     headers = ['_'.join(h.lower().split()) for h in headers]\n",
    "\n",
    "# df = pd.DataFrame(columns = headers,data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7f2b0d-bdcb-4dd8-ac2c-ddbdf61a0744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "97bf37b8-01d3-49c4-9cc7-315ebe6549d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_df[index:][final_df['doc_file_url'].str.contains('.aspx')]"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "nhanes",
   "name": "pytorch-gpu.2-0.m112",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.2-0:m112"
  },
  "kernelspec": {
   "display_name": "nhanes",
   "language": "python",
   "name": "nhanes"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
